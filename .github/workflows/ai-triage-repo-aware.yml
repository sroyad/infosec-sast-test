name: "Repo-Aware CodeQL AI Triage"

on:
  workflow_dispatch:
  schedule:
    - cron: "17 3 * * 1"  # Mondays 03:17 UTC

permissions:
  contents: read
  security-events: write
  actions: read

jobs:
  build-context-pack:
    name: Build Context Pack
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          python -m pip install regex tqdm

      - name: Generate repo context (summary + file cards)
        shell: bash
        run: |
          python - << 'PY'
          import os, re, json, pathlib
          from tqdm import tqdm

          ROOT = pathlib.Path(".")
          OUT  = pathlib.Path("context")
          OUT.mkdir(parents=True, exist_ok=True)

          CODE_EXTS = {".js",".ts",".jsx",".tsx",".py",".java",".kt",".go",".rb",".php",".cs",".scala",".rs",".c",".cpp"}
          IGNORE_SEG = {"node_modules","vendor","dist","build","coverage","__tests__","tests","test",".git",".github",".next",".cache"}

          def want(path: pathlib.Path)->bool:
            if not path.is_file(): return False
            if path.suffix.lower() not in CODE_EXTS: return False
            for seg in path.parts:
              if seg in IGNORE_SEG: return False
            try:
              if path.stat().st_size > 300_000: return False
            except: return False
            return True

          def read_text(p: pathlib.Path)->str:
            try: return p.read_text(encoding="utf-8", errors="ignore")
            except: return ""

          files = [p for p in ROOT.rglob("*") if want(p)]
          all_code = "\n".join(read_text(p) for p in files[:5000])

          def has(pat): return re.search(pat, all_code, re.I) is not None

          auth_model = "unknown"
          if has(r'\bAuthorization\b') or has(r'\breq\.headers\.authorization\b'):
            auth_model = "header-jwt"
          if has(r'cookie') and has(r'jwt'):
            auth_model = "cookie-jwt"
          if has(r'session|express-session|flask\.session|django\.contrib\.sessions'):
            auth_model = "session-cookie"

          frameworks = []
          for name, sig in [
            ("express", r'\brequire\([\'"]express[\'"]\)|from\s+["\']express["\']'),
            ("fastapi", r'\bFastAPI\('),
            ("flask", r'\bfrom\s+flask\s+import\b|\bFlask\('),
            ("spring", r'org\.springframework'),
            ("django", r'\bdjango\.'),
            ("rails", r'\brails\b'),
            ("gin", r'\bgithub\.com/gin-gonic/gin'),
          ]:
            if has(sig): frameworks.append(name)

          protections = {
            "csrf": "present" if has(r'csrf|Csrf') else "unknown",
            "cors": "present" if has(r'\bcors\b') else "unknown",
            "xss_sanitize": "present" if has(r'sanitize|dompurify|bleach|html\.escape|xss') else "unknown",
            "ssrf_controls": "present" if has(r'allowlist|whitelist|http(s)?\.Agent|proxy') else "unknown",
          }

          repo_summary = {
            "auth_model": auth_model,
            "frameworks": sorted(set(frameworks)),
            "protections": protections,
            "notes": "Heuristic POC; extend per language/framework."
          }
          (OUT/"repo_summary.json").write_text(json.dumps(repo_summary, indent=2), encoding="utf-8")

          def extract_endpoints(txt: str, suffix: str):
            endpoints = []
            if suffix in (".js",".ts",".jsx",".tsx"):
              for m in re.finditer(r'\b(app|router)\.(get|post|put|patch|delete)\(\s*["\']([^"\']+)', txt, re.I):
                endpoints.append({"path": m.group(3), "method": m.group(2).upper()})
            if suffix == ".py":
              for m in re.finditer(r'@(app|router)\.(get|post|put|patch|delete)\(\s*["\']([^"\']+)', txt, re.I):
                endpoints.append({"path": m.group(3), "method": m.group(2).upper()})
              for m in re.finditer(r'@app\.route\(\s*["\']([^"\']+).*methods=\[([^\]]+)\]', txt, re.I):
                meths = [s.strip(" '\"").upper() for s in m.group(2).split(",")]
                for mm in meths:
                  endpoints.append({"path": m.group(1), "method": mm})
            return endpoints[:12]

          def extract_imports(txt: str, suffix: str):
            imps = []
            if suffix in (".js",".ts",".jsx",".tsx"):
              imps += [m.group(1) for m in re.finditer(r'from\s+["\']([^"\']+)["\']', txt)]
              imps += [m.group(1) for m in re.finditer(r'require\(\s*["\']([^"\']+)["\']\s*\)', txt)]
            elif suffix == ".py":
              imps += [m.group(1) for m in re.finditer(r'^\s*from\s+([a-zA-Z0-9_\.]+)\s+import', txt, re.M)]
              imps += [m.group(1) for m in re.finditer(r'^\s*import\s+([a-zA-Z0-9_\.]+)', txt, re.M)]
            return list(dict.fromkeys(imps))[:30]

          def extract_sinks_sources(txt: str):
            sinks, sources = set(), set()
            for kw in ["exec(", "spawn(", "child_process", "Runtime.getRuntime()", "subprocess", "ProcessBuilder", "os.system", "popen"]:
              if kw in txt: sinks.add("command_exec")
            for kw in ["SELECT ", "insert(", "update(", ".where(", "execute(", "cursor.execute(", "ORM", "Session("]:
              if kw.lower() in txt.lower(): sinks.add("db")
            for kw in ["render(", "template", "Jinja2", "Handlebars", "ejs", "mustache"]:
              if kw.lower() in txt.lower(): sinks.add("template_render")
            for kw in ["req.body", "req.query", "req.params", "request.form", "request.args", "request.json", "ctx.Query", "ctx.PostForm"]:
              if kw in txt: sources.add("user_input")
            return sorted(sinks)[:12], sorted(sources)[:12]

          def extract_sanitizers_validators(txt: str):
            san, val = set(), set()
            for kw in ["sanitize", "escape", "dompurify", "bleach", "html.escape", "strip_tags", "encode"]:
              if re.search(r'\b'+re.escape(kw)+r'\b', txt, re.I): san.add(kw)
            for kw in ["zod", "joi", "ajv", "pydantic", "marshmallow", "validator", "class-validator"]:
              if re.search(r'\b'+re.escape(kw)+r'\b', txt, re.I): val.add(kw)
            return sorted(san)[:12], sorted(val)[:12]

          with (OUT/"file_cards.jsonl").open("w", encoding="utf-8") as f:
            for p in tqdm(files, desc="cards"):
              txt = read_text(p)
              suffix = p.suffix.lower()
              endpoints = extract_endpoints(txt, suffix)
              imports   = extract_imports(txt, suffix)
              sinks, sources = extract_sinks_sources(txt)
              san, val  = extract_sanitizers_validators(txt)

              summary_bits = []
              if endpoints: summary_bits.append(f"{len(endpoints)} endpoints")
              if "Authorization" in txt: summary_bits.append("reads Authorization header")
              if "cookie" in txt.lower(): summary_bits.append("reads cookies")
              if san: summary_bits.append("sanitizers present")
              if val: summary_bits.append("validators present")
              summary = "; ".join(summary_bits) or "general module"

              row = {
                "path": str(p.as_posix()),
                "language": suffix.lstrip("."),
                "summary": summary,
                "endpoints": endpoints,
                "imports": imports,
                "sinks": sinks,
                "sources": sources,
                "sanitizers": san,
                "validators": val,
              }
              f.write(json.dumps(row, ensure_ascii=False) + "\n")

          print("Context pack built at ./context/")
          PY

      - name: Upload context-pack
        uses: actions/upload-artifact@v4
        with:
          name: context-pack
          path: context/
          if-no-files-found: error

  ai-triage:
    name: AI Triage (repo-aware)
    runs-on: ubuntu-latest
    needs: [build-context-pack]

    env:
      OWNER: ${{ github.repository_owner }}
      REPO:  ${{ github.event.repository.name }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      DEVSAI_API_KEY: ${{ secrets.DEVSAI_API_KEY }}
      DEVSAI_AI_ID:   "5c59c0bf-b78b-4c74-9787-e0bae6225bd5"
      DEVSAI_BASE_URL: "https://devs.ai"

      ALERT_STATE: "open"
      MAX_ALERTS: "200"
      AUTO_DISMISS: "false"
      DISMISS_REASON: "false positive"
      SAFE_PATH_HINTS: "test,__tests__,spec,dist,build,node_modules,vendor,generated,coverage,min.js"

    steps:
      - name: Checkout (for code slices)
        uses: actions/checkout@v4
        with: { fetch-depth: 0 }

      - name: Download context-pack
        uses: actions/download-artifact@v4
        with:
          name: context-pack
          path: context

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          python -m pip install requests pexpect

      - name: Ensure devs.py is present
        run: |
          test -f .github/tools/devs.py || { echo "::error::devs.py missing at .github/tools/devs.py"; exit 1; }

      - name: Configure devs.ai CLI (headless)
        run: |
          mkdir -p "$HOME/.devscli"
          cat > "$HOME/.devscli/config.json" <<JSON
          {"api_key":"${DEVSAI_API_KEY}","selected_ai_id":"${DEVSAI_AI_ID}","base_url":"${DEVSAI_BASE_URL}"}
          JSON

      - name: Repo-aware triage
        shell: bash
        run: |
          python - << 'PY'
          import os, json, base64, re, textwrap, requests, pexpect, pathlib

          OWNER=os.environ["OWNER"]; REPO=os.environ["REPO"]
          GH=os.environ["GITHUB_TOKEN"]
          ALERT_STATE=os.getenv("ALERT_STATE","open")
          MAX_ALERTS=int(os.getenv("MAX_ALERTS","200"))
          AUTO_DISMISS=os.getenv("AUTO_DISMISS","false").lower()=="true"
          DISMISS_REASON=os.getenv("DISMISS_REASON","false positive")
          SAFE_HINTS=[s.strip().lower() for s in os.getenv("SAFE_PATH_HINTS","").split(",") if s.strip()]
          SUMMARY_PATH=os.environ.get("GITHUB_STEP_SUMMARY")
          DEVSPY=".github/tools/devs.py"

          # ---- load context ----
          repo_summary = {}
          file_cards = []
          try:
            repo_summary = json.load(open("context/repo_summary.json","r",encoding="utf-8"))
          except FileNotFoundError:
            repo_summary = {"notes":"missing repo_summary.json"}
          try:
            with open("context/file_cards.jsonl","r",encoding="utf-8") as f:
              for line in f:
                line=line.strip()
                if line:
                  file_cards.append(json.loads(line))
          except FileNotFoundError:
            pass

          by_path = {c["path"]: c for c in file_cards}

          sess=requests.Session()
          sess.headers.update({"Authorization": f"Bearer {GH}", "Accept":"application/vnd.github+json"})

          def list_alerts(state, page, per_page):
            url=f"https://api.github.com/repos/{OWNER}/{REPO}/code-scanning/alerts"
            r=sess.get(url, params={"state":state, "page":page, "per_page":per_page, "sort":"created","direction":"desc"}, timeout=60)
            r.raise_for_status(); return r.json()

          def get_file_snippet(path, ref_sha, start, end, ctx=10):
            if not path: return ""
            url=f"https://api.github.com/repos/{OWNER}/{REPO}/contents/{path}"
            r=sess.get(url, params={"ref":ref_sha or "HEAD"}, timeout=60)
            if r.status_code!=200: return ""
            j=r.json()
            content=base64.b64decode(j.get("content","")).decode("utf-8","ignore")
            lines=content.splitlines()
            s=max(1,(start or 1)-ctx); e=min(len(lines),(end or start or 1)+ctx)
            return "\n".join(f"{i+1:>5}: {lines[i]}" for i in range(s-1,e))

          SEC_KEYWORDS = set("render escape encode sanitize template csrf cors jwt cookie header auth session sql query orm exec spawn subprocess popen file upload s3 http axios fetch request urllib xml deserialize serialize pickle jackson gson yaml load proxy dns socket".split())

          def keyword_score(text):
            t=text.lower()
            return sum(1 for k in SEC_KEYWORDS if k in t)

          def related_cards(main_path, alert_msg, code_slice, k=5):
            cards=[]
            c = by_path.get(main_path)
            if c: cards.append(c)
            prefix = str(pathlib.Path(main_path).parent).rstrip("/")
            if prefix and prefix!=".":
              neigh = [fc for fc in file_cards if fc["path"].startswith(prefix+"/")]
              neigh = sorted(neigh, key=lambda x: -keyword_score(x.get("summary","")+" "+ " ".join(x.get("imports",[]))))
              cards.extend(neigh[:2])
            basis = (alert_msg or "") + " " + (code_slice or "")
            ranked = sorted(file_cards, key=lambda x: -keyword_score((x.get("summary","")+" "+ " ".join(x.get("imports",[]))+" "+ " ".join(x.get("sinks",[]))+" "+ " ".join(x.get("sources",[]))).lower() + " " + basis.lower()))
            for r in ranked[:5]:
              if r not in cards:
                cards.append(r)
            seen=set(); uniq=[]
            for x in cards:
              key=x["path"]
              if key in seen: continue
              seen.add(key); uniq.append(x)
              if len(uniq)>=k: break
            return uniq

          def prompt_for_alert(a, snippet, bundle_cards):
            rule=a.get("rule",{}) or {}
            tool=(a.get("tool") or {}).get("name","")
            rule_id=rule.get("id",""); rule_name=rule.get("name","")
            sev=(a.get("rule_severity") or a.get("severity") or "")
            inst=a.get("most_recent_instance") or {}
            loc=(inst.get("location") or {})
            path=loc.get("path",""); start=loc.get("start_line"); msg=inst.get("message")
            msg = (msg or {}).get("text") if isinstance(msg,dict) else (msg or "")

            cards_block = "\n".join([
              json.dumps({
                "path": c["path"],
                "summary": c.get("summary",""),
                "endpoints": c.get("endpoints",[]),
                "sanitizers": c.get("sanitizers",[]),
                "validators": c.get("validators",[]),
                "imports": c.get("imports",[]),
                "sinks": c.get("sinks",[]),
                "sources": c.get("sources",[])
              }, ensure_ascii=False) for c in bundle_cards
            ])

            example = {
              "classification": "TP",
              "certainty": 92,
              "rationale": "Untrusted input reaches query without validation; no sanitizer in path.",
              "evidence": [{"path": "app/db.js", "lines": "47-59", "reason":"query built from req.body"}],
              "reproduce_steps": "POST /api/search with crafted body",
              "fix_suggestion": "Use schema validation / parameterized query."
            }

            return textwrap.dedent(f"""
            You are a senior AppSec engineer doing rule-agnostic triage of a CodeQL alert with whole-repo context.

            Return ONLY a single JSON object with fields:
            - classification: "TP" | "FP" | "UNCERTAIN"
            - certainty: integer 0-100
            - rationale: short text
            - evidence: array of objects {{ "path": "...", "lines": "optional", "reason": "..." }}
            - reproduce_steps: short text or null
            - fix_suggestion: short text

            Example output:
            ```json
            {json.dumps(example, ensure_ascii=False)}
            ```

            Global repo summary:
            {json.dumps(repo_summary, ensure_ascii=False)}

            Context cards (subset of repo):
            {cards_block}

            Alert:
            - tool: {tool or 'CodeQL'}
            - ruleId: {rule_id}
            - ruleName: {rule_name}
            - severity: {sev}
            - file: {path}:{start}
            - message: {msg}

            Code slice (±10 lines):
            ```
            {snippet}
            ```
            """)

          def call_devs(prompt: str) -> str:
            child = pexpect.spawn("python3", [DEVSPY], encoding="utf-8", timeout=240)
            try:
              child.expect(r'Chat started', timeout=60)
            except Exception:
              pass
            child.sendline(prompt)
            buf=[]
            while True:
              try:
                line = child.readline()
              except Exception:
                break
              if not line:
                break
              if line.strip().startswith("> "):
                break
              buf.append(line)
            try:
              child.sendline("/exit")
              child.expect(pexpect.EOF, timeout=5)
            except Exception:
              pass
            return "".join(buf).strip()

          # --- JSON extraction helpers (no PCRE recursion) ---
          FENCED_JSON_RX = re.compile(r"```json\s*(\{.*?\})\s*```", re.S|re.I)

          def first_json_object(text: str, max_len: int = 200_000):
            """Find the first balanced JSON object by scanning braces."""
            if not text: return None
            text = text if len(text) <= max_len else text[:max_len]
            m = FENCED_JSON_RX.search(text)
            if m:
              try: return json.loads(m.group(1))
              except Exception: pass
            opens = [i for i,ch in enumerate(text) if ch == "{"]
            for start in opens:
              depth = 0
              in_str = False
              escape = False
              for i in range(start, len(text)):
                ch = text[i]
                if in_str:
                  if escape: escape = False
                  elif ch == "\\": escape = True
                  elif ch == '"': in_str = False
                else:
                  if ch == '"': in_str = True
                  elif ch == "{": depth += 1
                  elif ch == "}":
                    depth -= 1
                    if depth == 0:
                      candidate = text[start:i+1]
                      try: return json.loads(candidate)
                      except Exception: break
            return None

          VERDICT_RX = re.compile(r'^\s*(?:Verdict|Decision|Assessment|Triage\s*decision)\s*:\s*((?:Likely\s+)?(?:REAL_ISSUE|FALSE_POSITIVE))\b', re.I|re.M)
          def map_old_verdict(tok: str):
            t=tok.upper().strip()
            likely=t.startswith("LIKELY ")
            base=t.replace("LIKELY ","")
            if base=="REAL_ISSUE": return ("TP", 90 if not likely else 75)
            if base=="FALSE_POSITIVE": return ("FP", 90 if not likely else 75)
            return ("UNCERTAIN", 50)

          def parse_ai_reply(text: str):
            obj = first_json_object(text)
            if obj is not None:
              return obj
            for m in VERDICT_RX.finditer(text or ""):
              classification, certainty = map_old_verdict(m.group(1))
              return {
                "classification": classification,
                "certainty": certainty,
                "rationale": (text[m.end():].strip() or "(no explanation)")[:1000],
                "evidence": [], "reproduce_steps": None, "fix_suggestion": ""
              }
            return {"classification":"UNCERTAIN","certainty":50,"rationale":"No JSON/verdict found","evidence":[],"reproduce_steps":None,"fix_suggestion":""}

          def safe_path(p: str) -> bool:
            p = (p or "").lower()
            return any(h in p for h in SAFE_HINTS)

          # ---- fetch alerts ----
          alerts=[]; page=1
          while len(alerts) < MAX_ALERTS:
            batch = list_alerts(ALERT_STATE, page, min(100, MAX_ALERTS - len(alerts)))
            if not batch: break
            alerts.extend(batch); page += 1
          alerts = [a for a in alerts if (a.get("tool") or {}).get("name","").lower()=="codeql"]

          triaged=[]
          for a in alerts:
            inst = a.get("most_recent_instance") or {}
            loc = (inst.get("location") or {})
            path = loc.get("path") or ""
            start = loc.get("start_line") or 1
            end   = loc.get("end_line") or start
            ref_sha = inst.get("commit_sha") or a.get("most_recent_analysis_commit_sha") or "HEAD"

            snippet = get_file_snippet(path, ref_sha, start, end)
            bundle = related_cards(path, (inst.get("message") or {}).get("text",""), snippet, k=5)
            prompt = prompt_for_alert(a, snippet, bundle)

            raw = call_devs(prompt)
            verdict = parse_ai_reply(raw)

            classification = str(verdict.get("classification","UNCERTAIN")).upper()
            certainty = int(verdict.get("certainty", 50))
            rationale = (verdict.get("rationale") or "")[:1000]

            rec = {
              "alert_number": a.get("number"),
              "rule_id": (a.get("rule") or {}).get("id"),
              "severity": a.get("rule_severity") or a.get("severity"),
              "path": path,
              "classification": classification,
              "certainty": certainty,
              "rationale": rationale,
              "dismissed": False
            }

            if AUTO_DISMISS and classification=="FP" and certainty>=90 and safe_path(path):
              url = f"https://api.github.com/repos/{OWNER}/{REPO}/code-scanning/alerts/{a.get('number')}"
              r = sess.patch(url, json={"state":"dismissed","dismissed_reason":DISMISS_REASON}, timeout=60)
              rec["dismissed"] = r.status_code in (200,201)

            triaged.append(rec)

          lines = [
            f"Repo-aware triage complete.",
            f"Scope: state={ALERT_STATE} | Reviewed: {len(triaged)} | Auto-dismiss: {'ON' if AUTO_DISMISS else 'OFF'}",
            "",
            "| Alert # | Rule | Sev | Path | Class | Cert | Dismissed |",
            "|---:|---|:---:|---|:---:|:--:|:--:|",
          ]
          for r in triaged:
            lines.append(f"| {r['alert_number']} | `{r['rule_id']}` | {r['severity'] or ''} | `{r['path']}` | **{r['classification']}** | {r['certainty']} | {'✔️' if r['dismissed'] else ''} |")

          if SUMMARY_PATH:
            with open(SUMMARY_PATH,"a",encoding="utf-8") as f:
              f.write("\n".join(lines)+"\n")

          with open("repo-aware-triage.json","w",encoding="utf-8") as f:
            json.dump(triaged, f, indent=2)
          print("Wrote repo-aware-triage.json")
          PY

      - name: Upload triage JSON
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: repo-aware-triage
          path: repo-aware-triage.json
          if-no-files-found: error
